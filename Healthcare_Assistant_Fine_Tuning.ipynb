{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3789fe1",
      "metadata": {
        "id": "a3789fe1"
      },
      "source": [
        "# Healthcare Assistant via LLM Fine-Tuning\n",
        "\n",
        "## Project Overview\n",
        "This notebook fine-tunes **TinyLlama-1.1B-Chat** on medical flashcard Q&A data to create a domain-specific healthcare assistant.\n",
        "\n",
        "**Domain**: Healthcare (Medical Question Answering)  \n",
        "**Problem**: General LLMs lack specialized medical knowledge for accurate healthcare Q&A  \n",
        "**Solution**: Fine-tune TinyLlama on medical flashcards using LoRA for domain-specific accuracy\n",
        "\n",
        "**Key Features:**\n",
        "- **Domain**: Healthcare (Medical Q&A)\n",
        "- **Model**: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B parameters)\n",
        "- **Dataset**: medalpaca/medical_meadow_medical_flashcards (10,178 samples)\n",
        "- **Method**: LoRA (Low-Rank Adaptation) via PEFT library\n",
        "- **Platform**: Google Colab (Free T4 GPU)\n",
        "- **Training Samples**: 2,000 (validation: 200)\n",
        "\n",
        "**Assignment Goals:**\n",
        "1. Fine-tune a generative LLM for healthcare domain\n",
        "\n",
        "2. Implement parameter-efficient training (LoRA + 4-bit quantization)6. Document 3 hyperparameter experiments\n",
        "\n",
        "3. Evaluate with NLP metrics (BLEU, ROUGE, perplexity)5. Deploy with Gradio UI\n",
        "4. Compare base vs fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d55a4645",
      "metadata": {
        "id": "d55a4645"
      },
      "source": [
        "---\n",
        "## 1. Environment Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ab98fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ab98fe",
        "outputId": "cab0ada3-c7ff-4ebc-e529-5317f9f019a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch version: 2.10.0+cu128\n",
            "[OK] Running in Google Colab\n",
            "[OK] GPU Available: Tesla T4\n",
            "  GPU Memory: 15.64 GB\n"
          ]
        }
      ],
      "source": [
        "# Check environment and GPU\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Verify we're in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    print(\"[OK] Running in Google Colab\")\n",
        "except:\n",
        "    print(\"[WARNING] Not running in Google Colab. This notebook is optimized for Colab.\")\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"[OK] GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"[INFO] No GPU detected. Training will use CPU (slower).\")\n",
        "    print(\"   Recommendation: Enable GPU in Runtime > Change runtime type > Hardware accelerator > T4 GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9981c6fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9981c6fb",
        "outputId": "8dd28543-9df7-411e-b48b-8abdf9e47ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "\n",
            "\n",
            "[OK] All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for Colab\n",
        "print(\"Installing dependencies...\\n\")\n",
        "\n",
        "!pip install -q -U datasets transformers accelerate peft bitsandbytes\n",
        "!pip install -q -U evaluate rouge-score sacrebleu gradio\n",
        "\n",
        "print(\"\\n[OK] All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"-q\", \"pyarrow\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"-q\", \"datasets\"])\n",
        "print(\"[OK] Fixed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H3qx25zeo-P",
        "outputId": "a17602f5-1175-4383-e53b-d2a3e4eb0278"
      },
      "id": "7H3qx25zeo-P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Fixed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX: Aggressive PyArrow & Datasets compatibility resolver\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"AGGRESSIVE FIX: Resolving PyArrow binary compatibility issue...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Clear pip cache\n",
        "print(\"\\n[Step 1] Clearing pip cache...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"], capture_output=True)\n",
        "print(\"✓ Cache cleared\")\n",
        "\n",
        "# Step 2: Completely uninstall problematic packages\n",
        "print(\"\\n[Step 2] Uninstalling conflicting packages...\")\n",
        "for package in [\"pyarrow\", \"datasets\", \"transformers\", \"huggingface-hub\"]:\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package],\n",
        "        capture_output=True\n",
        "    )\n",
        "print(\"✓ Packages removed\")\n",
        "\n",
        "# Step 3: Reinstall with compatible versions\n",
        "print(\"\\n[Step 3] Installing compatible versions...\")\n",
        "packages_to_install = [\n",
        "    \"pyarrow==14.0.1\",  # Specific stable version\n",
        "    \"datasets>=2.17.0\",\n",
        "    \"huggingface-hub>=0.20.0\",\n",
        "    \"transformers>=4.36.0\"\n",
        "]\n",
        "\n",
        "for package in packages_to_install:\n",
        "    print(f\"  Installing {package}...\")\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        print(f\"  ⚠ Warning: {result.stderr[:100]}\")\n",
        "    else:\n",
        "        print(f\"  ✓ {package} installed\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"[OK] AGGRESSIVE FIX COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n IMPORTANT: Restart the kernel now!\")\n",
        "print(\"   In Colab: Runtime > Restart runtime\")\n",
        "print(\"   In VS Code: Select Restart Kernel button\")\n",
        "print(\"\\nThen run the 'Import libraries' cell after restart.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUuUtftnfayL",
        "outputId": "b6eaa786-ccab-4e9d-ac90-1b847b48b20a"
      },
      "id": "hUuUtftnfayL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "AGGRESSIVE FIX: Resolving PyArrow binary compatibility issue...\n",
            "======================================================================\n",
            "\n",
            "[Step 1] Clearing pip cache...\n",
            "✓ Cache cleared\n",
            "\n",
            "[Step 2] Uninstalling conflicting packages...\n",
            "✓ Packages removed\n",
            "\n",
            "[Step 3] Installing compatible versions...\n",
            "  Installing pyarrow==14.0.1...\n",
            "  ✓ pyarrow==14.0.1 installed\n",
            "  Installing datasets>=2.17.0...\n",
            "  ✓ datasets>=2.17.0 installed\n",
            "  Installing huggingface-hub>=0.20.0...\n",
            "  ✓ huggingface-hub>=0.20.0 installed\n",
            "  Installing transformers>=4.36.0...\n",
            "  ✓ transformers>=4.36.0 installed\n",
            "\n",
            "======================================================================\n",
            "[OK] AGGRESSIVE FIX COMPLETE!\n",
            "======================================================================\n",
            "\n",
            " IMPORTANT: Restart the kernel now!\n",
            "   In Colab: Runtime > Restart runtime\n",
            "   In VS Code: Select Restart Kernel button\n",
            "\n",
            "Then run the 'Import libraries' cell after restart.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93e744a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c93e744a",
        "outputId": "2fff9c9f-bf44-47da-def1-d38adbe35ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"[OK] All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb137b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cb137b8",
        "outputId": "9d9a416f-0405-4c9e-a22e-3b6b0b44eca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Using device: cuda\n",
            "  GPU: Tesla T4\n",
            "  Available memory: 15.64 GB\n"
          ]
        }
      ],
      "source": [
        "# Check device (GPU or CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"[OK] Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"  [WARNING] Running on CPU. Training will be significantly slower.\")\n",
        "    print(\"  [RECOMMENDATION] Enable GPU: Runtime > Change runtime type > T4 GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b6c3e1",
      "metadata": {
        "id": "c7b6c3e1"
      },
      "source": [
        "---\n",
        "## 2. Dataset Loading & Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05fa59e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b05fa59e",
        "outputId": "93ee6699-0051-4f98-a177-b0b153817297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: medalpaca/medical_meadow_medical_flashcards\n",
            "\n",
            "Dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input', 'output', 'instruction'],\n",
            "        num_rows: 33955\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load the medical flashcards dataset\n",
        "print(\"Loading dataset: medalpaca/medical_meadow_medical_flashcards\")\n",
        "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f7cee9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12f7cee9",
        "outputId": "c669703b-a763-433c-8567-bbbe64a29e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info:\n",
            "Total samples: 33955\n",
            "\n",
            "Columns: ['input', 'output', 'instruction']\n",
            "\n",
            "First 3 examples:\n",
            "\n",
            "--- Example 1 ---\n",
            "input: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
            "output: Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.\n",
            "instruction: Answer this question truthfully\n",
            "\n",
            "--- Example 2 ---\n",
            "input: What leads to genitourinary syndrome of menopause (atrophic vaginitis)?\n",
            "output: Low estradiol production leads to genitourinary syndrome of menopause (atrophic vaginitis).\n",
            "instruction: Answer this question truthfully\n",
            "\n",
            "--- Example 3 ---\n",
            "input: What does low REM sleep latency and experiencing hallucinations/sleep paralysis suggest?\n",
            "output: Low REM sleep latency and experiencing hallucinations/sleep paralysis suggests narcolepsy.\n",
            "instruction: Answer this question truthfully\n"
          ]
        }
      ],
      "source": [
        "# Explore dataset\n",
        "print(\"Dataset info:\")\n",
        "print(f\"Total samples: {len(dataset['train'])}\")\n",
        "print(f\"\\nColumns: {dataset['train'].column_names}\")\n",
        "print(f\"\\nFirst 3 examples:\")\n",
        "\n",
        "for i in range(3):\n",
        "    example = dataset['train'][i]\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    for key, value in example.items():\n",
        "        print(f\"{key}: {value[:200] if isinstance(value, str) and len(value) > 200 else value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5225683b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "5225683b",
        "outputId": "8f31d762-d06a-4ac0-be88-8ea7ee35cdef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length statistics:\n",
            "Mean: 441.50 characters\n",
            "Median: 265.00 characters\n",
            "Max: 1689 characters\n",
            "Min: 0 characters\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Analyze dataset statistics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get all text lengths\n",
        "text_lengths = []\n",
        "for item in dataset['train']:\n",
        "    # Adjust field name based on actual dataset structure\n",
        "    if 'input' in item and 'output' in item:\n",
        "        text_lengths.append(len(item['input']) + len(item['output']))\n",
        "    elif 'instruction' in item and 'output' in item:\n",
        "        text_lengths.append(len(item['instruction']) + len(item['output']))\n",
        "\n",
        "print(f\"Text length statistics:\")\n",
        "print(f\"Mean: {np.mean(text_lengths):.2f} characters\")\n",
        "print(f\"Median: {np.median(text_lengths):.2f} characters\")\n",
        "print(f\"Max: {np.max(text_lengths)} characters\")\n",
        "print(f\"Min: {np.min(text_lengths)} characters\")\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(text_lengths, bins=50, edgecolor='black')\n",
        "plt.xlabel('Text Length (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Text Lengths in Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a737f061",
      "metadata": {
        "id": "a737f061"
      },
      "source": [
        "---\n",
        "## 3. Data Preprocessing & Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1be70b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1be70b7",
        "outputId": "74be7780-83c5-43f3-d4a5-7c6632fa3239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample formatted prompt:\n",
            "<|system|>\n",
            "You are a helpful medical assistant. Provide accurate and concise answers to medical questions.\n",
            "<|user|>\n",
            "What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
            "<|assistant|>\n",
            "Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.\n",
            "\n",
            "Prompt length: 305 characters\n"
          ]
        }
      ],
      "source": [
        "# Define prompt template for instruction-following format\n",
        "def create_prompt_template(instruction, response=\"\"):\n",
        "    \"\"\"\n",
        "    Create a chat-style prompt for TinyLlama.\n",
        "    Format: <|system|>\\n{system_message}\\n<|user|>\\n{instruction}\\n<|assistant|>\\n{response}\n",
        "    \"\"\"\n",
        "    system_message = \"You are a helpful medical assistant. Provide accurate and concise answers to medical questions.\"\n",
        "\n",
        "    if response:\n",
        "        # Training format (with answer)\n",
        "        prompt = f\"\"\"<|system|>\n",
        "{system_message}\n",
        "<|user|>\n",
        "{instruction}\n",
        "<|assistant|>\n",
        "{response}\"\"\"\n",
        "    else:\n",
        "        # Inference format (without answer)\n",
        "        prompt = f\"\"\"<|system|>\n",
        "{system_message}\n",
        "<|user|>\n",
        "{instruction}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Test the template\n",
        "sample = dataset['train'][0]\n",
        "# Adjust field names based on actual dataset structure\n",
        "instruction_field = 'input' if 'input' in sample else 'instruction'\n",
        "output_field = 'output'\n",
        "\n",
        "test_prompt = create_prompt_template(sample[instruction_field], sample[output_field])\n",
        "print(\"Sample formatted prompt:\")\n",
        "print(test_prompt)\n",
        "print(f\"\\nPrompt length: {len(test_prompt)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c73f23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c73f23",
        "outputId": "d4e1212d-0f66-4e6b-990c-bb559bcf0038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "[OK] Tokenizer loaded\n",
            "Vocabulary size: 32000\n",
            "Max length: 2048\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "print(f\"Loading tokenizer: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"[OK] Tokenizer loaded\")\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Max length: {tokenizer.model_max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22dca03f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dca03f",
        "outputId": "5e50dabd-032c-4125-a66a-3f622add2fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting dataset...\n",
            "[OK] Dataset formatted\n",
            "Sample formatted text:\n",
            "<|system|>\n",
            "You are a helpful medical assistant. Provide accurate and concise answers to medical questions.\n",
            "<|user|>\n",
            "What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
            "<|assistant|>\n",
            "Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels....\n"
          ]
        }
      ],
      "source": [
        "# Preprocess dataset\n",
        "def format_dataset(examples):\n",
        "    \"\"\"Format dataset into instruction-response pairs.\"\"\"\n",
        "    # Determine field names\n",
        "    sample_keys = list(examples.keys())\n",
        "    instruction_field = 'input' if 'input' in sample_keys else 'instruction'\n",
        "    output_field = 'output'\n",
        "\n",
        "    prompts = []\n",
        "    for i in range(len(examples[instruction_field])):\n",
        "        instruction = examples[instruction_field][i]\n",
        "        response = examples[output_field][i]\n",
        "        prompt = create_prompt_template(instruction, response)\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    return {\"text\": prompts}\n",
        "\n",
        "# Apply formatting\n",
        "print(\"Formatting dataset...\")\n",
        "formatted_dataset = dataset.map(\n",
        "    format_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names\n",
        ")\n",
        "\n",
        "print(\"[OK] Dataset formatted\")\n",
        "print(f\"Sample formatted text:\\n{formatted_dataset['train'][0]['text'][:500]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861eb7b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "861eb7b2",
        "outputId": "9f3ec11c-5b75-432e-ff05-9063087ec997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing dataset...\n",
            "[OK] Dataset tokenized\n",
            "Sample tokenized length: 88 tokens\n"
          ]
        }
      ],
      "source": [
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the formatted prompts.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,  # Limit to 512 tokens for faster training\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = formatted_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"[OK] Dataset tokenized\")\n",
        "print(f\"Sample tokenized length: {len(tokenized_dataset['train'][0]['input_ids'])} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f4a46c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11f4a46c",
        "outputId": "e4daeec0-c6b5-4759-a530-2a07b155d5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 2000\n",
            "Validation samples: 200\n"
          ]
        }
      ],
      "source": [
        "# Split dataset: use 2000 samples for training, 200 for validation\n",
        "# This balances quality with training time on free Colab\n",
        "train_size = 2000\n",
        "val_size = 200\n",
        "\n",
        "if len(tokenized_dataset['train']) < train_size + val_size:\n",
        "    train_size = int(len(tokenized_dataset['train']) * 0.9)\n",
        "    val_size = len(tokenized_dataset['train']) - train_size\n",
        "\n",
        "train_dataset = tokenized_dataset['train'].select(range(train_size))\n",
        "eval_dataset = tokenized_dataset['train'].select(range(train_size, train_size + val_size))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18c85a8",
      "metadata": {
        "id": "f18c85a8"
      },
      "source": [
        "---\n",
        "## 4. Model Loading with LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b17e2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "74536fb416864ab99c152f2d5f2cc909",
            "b810d861996f40c18169a26d1f985326",
            "d992bcbd0b40459ab320e481c2caed37",
            "357b2584712f4190a165120406398277",
            "41e661994c5c45b4ab5ebd0b6503547e",
            "24924df5711f48fd96c91101ac543d8b",
            "97233d832d804f1fa288562c5ca9fb59",
            "6e15bb23809b46589149bc461fb74be9",
            "fc663443002c4e059eec3478a6ba67ff",
            "279f88422e2d4ece889e307f436fa305",
            "50df82cb05fc49f8b013ebf49738bf84"
          ]
        },
        "id": "f4b17e2a",
        "outputId": "fa5d0475-6e27-46ee-9e1f-0353ed4cc5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Using 4-bit quantization for memory efficiency on GPU\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74536fb416864ab99c152f2d5f2cc909"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Model loaded with 4-bit quantization\n",
            "Model parameters: 1100.05M\n"
          ]
        }
      ],
      "source": [
        "# Load base model (with 4-bit quantization on GPU, full precision on CPU)\n",
        "print(f\"Loading base model: {model_name}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using 4-bit quantization for memory efficiency on GPU\")\n",
        "    from transformers import BitsAndBytesConfig\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load model with quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Prepare for LoRA training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    print(\"[OK] Model loaded with 4-bit quantization\")\n",
        "else:\n",
        "    print(\"Using full precision on CPU\")\n",
        "    # Load model without quantization for CPU\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"[OK] Model loaded in full precision\")\n",
        "\n",
        "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1653518c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1653518c",
        "outputId": "f640995b-fd9f-44a7-ee45-af27bb871250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring LoRA...\n",
            "[OK] LoRA configured\n",
            "Trainable parameters: 4.51M (0.73%)\n",
            "Total parameters: 620.11M\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "print(\"Configuring LoRA...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # LoRA rank (higher = more parameters, better quality but slower)\n",
        "    lora_alpha=32,  # LoRA scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply LoRA to attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"[OK] LoRA configured\")\n",
        "print(f\"Trainable parameters: {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Total parameters: {total_params / 1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbccfee7",
      "metadata": {
        "id": "bbccfee7"
      },
      "source": [
        "---\n",
        "## 5. Training Configuration & Experiments\n",
        "\n",
        "We will run **3 experiments** with different hyperparameters to find the best configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2290aef7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2290aef7",
        "outputId": "842f0cc1-50bc-4696-acc6-2bfca9adc38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment Configurations:\n",
            "======================================================================\n",
            "\n",
            "Experiment 1: Baseline\n",
            "  learning_rate: 0.0002\n",
            "  num_epochs: 1\n",
            "  batch_size: 2\n",
            "  gradient_accumulation_steps: 4\n",
            "  warmup_steps: 100\n",
            "\n",
            "Experiment 2: Lower LR\n",
            "  learning_rate: 0.0001\n",
            "  num_epochs: 2\n",
            "  batch_size: 2\n",
            "  gradient_accumulation_steps: 4\n",
            "  warmup_steps: 100\n",
            "\n",
            "Experiment 3: Optimized\n",
            "  learning_rate: 5e-05\n",
            "  num_epochs: 2\n",
            "  batch_size: 4\n",
            "  gradient_accumulation_steps: 2\n",
            "  warmup_steps: 50\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Define experiment configurations\n",
        "experiments = {\n",
        "    \"Experiment 1: Baseline\": {\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"num_epochs\": 1,\n",
        "        \"batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_steps\": 100,\n",
        "    },\n",
        "    \"Experiment 2: Lower LR\": {\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"num_epochs\": 2,\n",
        "        \"batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_steps\": 100,\n",
        "    },\n",
        "    \"Experiment 3: Optimized\": {\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"num_epochs\": 2,\n",
        "        \"batch_size\": 4,\n",
        "        \"gradient_accumulation_steps\": 2,\n",
        "        \"warmup_steps\": 50,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display experiments\n",
        "print(\"Experiment Configurations:\")\n",
        "print(\"=\"*70)\n",
        "for exp_name, config in experiments.items():\n",
        "    print(f\"\\n{exp_name}\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c5572f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c5572f",
        "outputId": "38005094-cbf8-43d7-b729-f51804e8fb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: Experiment 1: Baseline\n",
            "Configuration: {'learning_rate': 0.0002, 'num_epochs': 1, 'batch_size': 2, 'gradient_accumulation_steps': 4, 'warmup_steps': 100}\n"
          ]
        }
      ],
      "source": [
        "# Choose experiment to run (change this to run different experiments)\n",
        "CURRENT_EXPERIMENT = \"Experiment 1: Baseline\"  # Change to test different configs\n",
        "\n",
        "config = experiments[CURRENT_EXPERIMENT]\n",
        "print(f\"Running: {CURRENT_EXPERIMENT}\")\n",
        "print(f\"Configuration: {config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871414a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "871414a2",
        "outputId": "b3b5caef-e2e6-4591-aea7-1320e58ad4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Training arguments configured\n"
          ]
        }
      ],
      "source": [
        "# Set up training arguments\n",
        "output_dir = f\"./results_{CURRENT_EXPERIMENT.split(':')[0].replace(' ', '_').lower()}\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=config[\"num_epochs\"],\n",
        "    per_device_train_batch_size=config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=config[\"batch_size\"],\n",
        "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "    learning_rate=config[\"learning_rate\"],\n",
        "    warmup_steps=config[\"warmup_steps\"],\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),  # Use FP16 on GPU, FP32 on CPU\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "print(\"[OK] Training arguments configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67183b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a67183b6",
        "outputId": "55581d55-1f7e-4755-e2eb-7f093c77b5ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Data collator ready\n"
          ]
        }
      ],
      "source": [
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal LM, not masked LM\n",
        ")\n",
        "\n",
        "print(\"[OK] Data collator ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a679b4b5",
      "metadata": {
        "id": "a679b4b5"
      },
      "source": [
        "---\n",
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e33a2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61e33a2a",
        "outputId": "f85d5b40-ab15-4d53-c3be-cf42f928961c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Trainer initialized\n",
            "Training will start on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"[OK] Trainer initialized\")\n",
        "print(f\"Training will start on device: {training_args.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a3bee39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "0a3bee39",
        "outputId": "409e6a8b-5b6f-4977-d9d4-ea529d701de2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting training: Experiment 1: Baseline\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='127' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [127/250 03:59 < 03:55, 0.52 it/s, Epoch 0.50/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:44, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.770235</td>\n",
              "      <td>0.737767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OK] Training completed!\n",
            "Training time: 7.82 minutes\n",
            "Peak GPU memory: 4.37 GB\n",
            "\n",
            "Final training loss: 0.8886\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Starting training: {CURRENT_EXPERIMENT}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Track GPU memory before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Track GPU memory after training\n",
        "if torch.cuda.is_available():\n",
        "    memory_peak = torch.cuda.max_memory_allocated() / 1e9\n",
        "    print(f\"\\n[OK] Training completed!\")\n",
        "    print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "    print(f\"Peak GPU memory: {memory_peak:.2f} GB\")\n",
        "else:\n",
        "    print(f\"\\n[OK] Training completed!\")\n",
        "    print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "\n",
        "# Save training metrics\n",
        "metrics = train_result.metrics\n",
        "print(f\"\\nFinal training loss: {metrics['train_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20ae922",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a20ae922",
        "outputId": "65e3a717-b87b-4dbe-ae40-e1697e5a626e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Model saved to: ./fine_tuned_model_experiment_1\n"
          ]
        }
      ],
      "source": [
        "# Save the fine-tuned model\n",
        "model_save_path = f\"./fine_tuned_model_{CURRENT_EXPERIMENT.split(':')[0].replace(' ', '_').lower()}\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"[OK] Model saved to: {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9e332a",
      "metadata": {
        "id": "5f9e332a"
      },
      "source": [
        "---\n",
        "## 7. Evaluation with NLP Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deac240a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deac240a",
        "outputId": "25cf3149-fe33-4647-ba79-3876ac0b66ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Evaluation metrics loaded (BLEU, ROUGE)\n"
          ]
        }
      ],
      "source": [
        "# Load evaluation metrics\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "print(\"[OK] Evaluation metrics loaded (BLEU, ROUGE)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d288bf22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d288bf22",
        "outputId": "2d9cb499-406d-4fb0-e5bd-ecb3a9551bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for evaluation...\n",
            "Processing 0/50...\n",
            "Processing 10/50...\n",
            "Processing 20/50...\n",
            "Processing 30/50...\n",
            "Processing 40/50...\n",
            "[OK] Predictions generated\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions on validation set\n",
        "print(\"Generating predictions for evaluation...\")\n",
        "\n",
        "num_eval_samples = 50  # Evaluate on subset for speed\n",
        "eval_samples = eval_dataset.select(range(min(num_eval_samples, len(eval_dataset))))\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, sample in enumerate(eval_samples):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processing {i}/{num_eval_samples}...\")\n",
        "\n",
        "        # Get input without the response\n",
        "        input_ids = torch.tensor([sample['input_ids'][:256]]).to(model.device)  # Use first half as input\n",
        "\n",
        "        # Generate\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        reference = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        references.append(reference)\n",
        "\n",
        "print(\"[OK] Predictions generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0747a87c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0747a87c",
        "outputId": "11ab48fd-cb23-4bfe-9154-b4c077334a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "BLEU Score: 46.54\n",
            "\n",
            "ROUGE Scores:\n",
            "  ROUGE-1: 0.5857\n",
            "  ROUGE-2: 0.5804\n",
            "  ROUGE-L: 0.5856\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Calculate BLEU score\n",
        "bleu_results = bleu_metric.compute(\n",
        "    predictions=predictions,\n",
        "    references=[[ref] for ref in references]\n",
        ")\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_results = rouge_metric.compute(\n",
        "    predictions=predictions,\n",
        "    references=references\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBLEU Score: {bleu_results['score']:.2f}\")\n",
        "print(f\"\\nROUGE Scores:\")\n",
        "print(f\"  ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
        "print(f\"  ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
        "print(f\"  ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5039b83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "d5039b83",
        "outputId": "b86d1c70-d583-41fb-ba80-9e31ae6d07c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating perplexity...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 12:47]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.7362\n",
            "Perplexity: 2.09\n"
          ]
        }
      ],
      "source": [
        "# Calculate perplexity on validation set\n",
        "print(\"\\nCalculating perplexity...\")\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "perplexity = np.exp(eval_results['eval_loss'])\n",
        "\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28b6788",
      "metadata": {
        "id": "a28b6788"
      },
      "source": [
        "---\n",
        "## 8. Qualitative Testing & Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26299ca3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26299ca3",
        "outputId": "b36cda8b-a877-4600-9b0d-4b3662603fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qualitative Testing: Fine-Tuned Model\n",
            "======================================================================\n",
            "\n",
            "Q: What is the treatment for pneumonia?\n",
            "A: The treatment for pneumonia is antibiotics. This can include a variety of different antibiotics depending on the type of pneumonia being treated. The type of antibiotic chosen will depend on the specific type of pneumonia being treated, as well as the severity of the infection. Some common antibiotics used in the treatment of pneumonia include amoxicillin, cephalexin, and azithromycin. It is important to follow the prescribed dosage and duration of treatment for pneumonia, as this can help to prevent complications and improve the outcome of treatment.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Q: What are the symptoms of diabetes?\n",
            "A: The symptoms of diabetes include polyuria, polydipsia, and polydromic symptoms. Polyuria refers to the presence of excessive urine production, which can be caused by various factors, such as excessive thirst, urinary tract infections, and kidney disease. Polydipsia is the presence of excessive urine production, which can be caused by various factors, such as kidney disease, diabetes, and thirst. Polydromic symptoms refer to the presence of multiple symptoms, which can be caused by various factors, such as kidney disease, diabetes, and thirst. These symptoms can be difficult to distinguish, especially in the early\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Q: How is hypertension diagnosed?\n",
            "A: Hypertension is diagnosed by measuring blood pressure. A systolic blood pressure of less than 120 mm Hg or a diastolic blood pressure of less than 80 mm Hg is considered high blood pressure. Other symptoms may be present, such as headache, fatigue, and sweating. If these symptoms are present, it is recommended that a doctor make a diagnosis of hypertension. Treatment for hypertension may include lifestyle changes such as diet and exercise, medication, or a combination of both. If a medical condition is causing hypertension, such as diabetes or kidney disease, it is essential to seek treatment from a medical\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Q: What causes anemia?\n",
            "A: Iron deficiency is the cause of anemia. Iron deficiency occurs when there is a deficiency of iron in the blood, which can lead to anemia. Iron is essential for the formation of hemoglobin, which is a protein that carries oxygen in the blood. If there is a deficiency of iron in the body, the blood cannot produce enough hemoglobin, leading to anemia. Iron deficiency can be caused by a variety of factors, including malnutrition, pregnancy, and medication. It is important to get proper iron supplementation to prevent anemia and its associated health issues.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Q: What is the function of insulin?\n",
            "A: Insulin is a hormone that regulates blood glucose levels in the body. It is produced by the pancreas and acts by stimulating the cells in the pancreas to secrete glucose into the bloodstream. This process is essential for maintaining healthy blood sugar levels and preventing diabetes. Insulin is also important in regulating the body's response to glucose levels, which can help to maintain a stable blood glucose level. Additionally, insulin may have other functions in the body, such as regulating fat storage and helping to maintain a healthy weight. However, the precise function of insulin is still being studied and may change over time\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test questions for qualitative evaluation\n",
        "test_questions = [\n",
        "    \"What is the treatment for pneumonia?\",\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How is hypertension diagnosed?\",\n",
        "    \"What causes anemia?\",\n",
        "    \"What is the function of insulin?\",\n",
        "]\n",
        "\n",
        "print(\"Qualitative Testing: Fine-Tuned Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for question in test_questions:\n",
        "    prompt = create_prompt_template(question, \"\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response\n",
        "    if \"<|assistant|>\" in response:\n",
        "        answer = response.split(\"<|assistant|>\")[-1].strip()\n",
        "    else:\n",
        "        answer = response\n",
        "\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918edd2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "f6da899aef294c6ead3f3cd38ae4b0b6",
            "0d9d8fc6adcf48ca958fce2623fffe6c",
            "76587f5265124091a0311af2095054a8",
            "24f664be3be84f44b734851645e4299e",
            "d6206e3ebbfb4ea79d7cbd55dc45cf6a",
            "0a29e3594a6a45f38045519c1d7323f9",
            "45df8d4eed7540d28b09438cce6fe1f8",
            "e088d67adfea46db8148328b0717c639",
            "f1442e1104d24203b5092ece1bfd57d2",
            "cc34afd80a974e139bd8e566b5b32dba",
            "f799de452f0e4f8195ba498c3ea9cf86"
          ]
        },
        "id": "918edd2a",
        "outputId": "2f2277d4-fafc-44e8-d945-aa932f87e92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model for comparison...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6da899aef294c6ead3f3cd38ae4b0b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Base model loaded\n"
          ]
        }
      ],
      "source": [
        "# Load base model for comparison (without fine-tuning)\n",
        "print(\"Loading base model for comparison...\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "else:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "print(\"[OK] Base model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a23a91b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a23a91b7",
        "outputId": "48c1a8d0-0088-4bb0-9f86-96096cc4bc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison: Base Model vs Fine-Tuned Model\n",
            "======================================================================\n",
            "\n",
            "Question: What is the treatment for pneumonia?\n",
            "\n",
            "--- BASE MODEL ---\n",
            "The treatment for pneumonia is generally supportive care, which means focusing on providing comfort and keeping the patient hydrated. The following are some common treatments:\n",
            "\n",
            "1. Antibiotics: These medications are prescribed to fight bacterial infections that may cause pneumonia.\n",
            "\n",
            "2. Supplemental oxygen: This helps to improve oxygenation in the blood and reduce inflammation.\n",
            "\n",
            "3. Antiviral medications: These medications are used to fight viral infections that may cause pneumonia.\n",
            "\n",
            "4. Hydration: Keeping the patient hydrated is essential for preventing dehydration, which can w\n",
            "\n",
            "--- FINE-TUNED MODEL ---\n",
            "Antibiotics are the treatment for pneumonia. This includes penicillin, amoxicillin, and cephalosporins. These antibiotics work by killing bacteria that are causing the infection. The type of antibiotic that is used depends on the type of bacteria that is causing the infection. Antibiotics can help to treat pneumonia by reducing the number of bacteria in the body and preventing further infection. However, it is important to take the antibiotics as prescribed by a medical professional to ensure that the treatment is effective.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Compare base vs fine-tuned\n",
        "print(\"\\nComparison: Base Model vs Fine-Tuned Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_question = \"What is the treatment for pneumonia?\"\n",
        "\n",
        "# Base model response\n",
        "prompt = create_prompt_template(comparison_question, \"\")\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "outputs_base = base_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "response_base = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n",
        "if \"<|assistant|>\" in response_base:\n",
        "    answer_base = response_base.split(\"<|assistant|>\")[-1].strip()\n",
        "else:\n",
        "    answer_base = response_base\n",
        "\n",
        "# Fine-tuned model response\n",
        "outputs_finetuned = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "response_finetuned = tokenizer.decode(outputs_finetuned[0], skip_special_tokens=True)\n",
        "if \"<|assistant|>\" in response_finetuned:\n",
        "    answer_finetuned = response_finetuned.split(\"<|assistant|>\")[-1].strip()\n",
        "else:\n",
        "    answer_finetuned = response_finetuned\n",
        "\n",
        "print(f\"\\nQuestion: {comparison_question}\")\n",
        "print(f\"\\n--- BASE MODEL ---\")\n",
        "print(answer_base)\n",
        "print(f\"\\n--- FINE-TUNED MODEL ---\")\n",
        "print(answer_finetuned)\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a43ae46",
      "metadata": {
        "id": "9a43ae46"
      },
      "source": [
        "---\n",
        "## 9. Experiment Tracking Table\n",
        "\n",
        "Document all experiments and their results for the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc61679",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fc61679",
        "outputId": "10a06823-8da1-429f-a8e7-6a8affe27c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EXPERIMENT TRACKING TABLE\n",
            "====================================================================================================\n",
            "            Experiment  Learning Rate  Epochs  Batch Size  Gradient Accumulation Training Loss Validation Loss Perplexity  BLEU ROUGE-1 ROUGE-L Training Time (min) Peak GPU Memory (GB)\n",
            "Experiment 1: Baseline         0.0002       1           2                      4        0.8886          0.7362       2.09 46.54  0.5857  0.5856                7.82                 4.37\n",
            "====================================================================================================\n",
            "\n",
            "[OK] Results saved to experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Create experiment tracking table\n",
        "experiment_results = {\n",
        "    \"Experiment\": [CURRENT_EXPERIMENT],\n",
        "    \"Learning Rate\": [config[\"learning_rate\"]],\n",
        "    \"Epochs\": [config[\"num_epochs\"]],\n",
        "    \"Batch Size\": [config[\"batch_size\"]],\n",
        "    \"Gradient Accumulation\": [config[\"gradient_accumulation_steps\"]],\n",
        "    \"Training Loss\": [f\"{metrics['train_loss']:.4f}\"],\n",
        "    \"Validation Loss\": [f\"{eval_results['eval_loss']:.4f}\"],\n",
        "    \"Perplexity\": [f\"{perplexity:.2f}\"],\n",
        "    \"BLEU\": [f\"{bleu_results['score']:.2f}\"],\n",
        "    \"ROUGE-1\": [f\"{rouge_results['rouge1']:.4f}\"],\n",
        "    \"ROUGE-L\": [f\"{rouge_results['rougeL']:.4f}\"],\n",
        "    \"Training Time (min)\": [f\"{training_time/60:.2f}\"],\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    experiment_results[\"Peak GPU Memory (GB)\"] = [f\"{memory_peak:.2f}\"]\n",
        "\n",
        "experiment_df = pd.DataFrame(experiment_results)\n",
        "\n",
        "print(\"\\nEXPERIMENT TRACKING TABLE\")\n",
        "print(\"=\"*100)\n",
        "print(experiment_df.to_string(index=False))\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Save to CSV\n",
        "experiment_df.to_csv(\"experiment_results.csv\", mode='a', header=not pd.io.common.file_exists(\"experiment_results.csv\"), index=False)\n",
        "print(\"\\n[OK] Results saved to experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21038d24",
      "metadata": {
        "id": "21038d24"
      },
      "source": [
        "---\n",
        "## 10. Gradio UI Deployment\n",
        "\n",
        "Create an interactive web interface for the healthcare assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a006df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25a006df",
        "outputId": "a0fbbc74-9cb5-462e-e606-c6eaa3cc259e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Gradio is already installed\n"
          ]
        }
      ],
      "source": [
        "# Install Gradio if not already installed\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    import gradio as gr\n",
        "    print(\"[OK] Gradio is already installed\")\n",
        "except:\n",
        "    print(\"Installing Gradio...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gradio\"])\n",
        "    import gradio as gr\n",
        "    print(\"[OK] Gradio installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chat function for Gradio - ULTRA FAST VERSION\n",
        "# Purpose: fastest possible responses (short + direct)\n",
        "\n",
        "def chat_with_assistant(message, history):\n",
        "    try:\n",
        "        model.eval()\n",
        "        model.config.use_cache = True\n",
        "\n",
        "        # Short prompt for speed\n",
        "        prompt = create_prompt_template(message, \"\")\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=256  # shorter context = faster\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = inputs.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # Ultra-fast decoding (greedy, short)\n",
        "        with torch.inference_mode():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=50,    # ultra short\n",
        "                min_new_tokens=10,\n",
        "                do_sample=False,      # greedy decoding (fastest)\n",
        "                temperature=0.0,\n",
        "                top_p=1.0,\n",
        "                top_k=0,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=2,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \"<|assistant|>\" in full_response:\n",
        "            response_text = full_response.split(\"<|assistant|>\")[-1].strip()\n",
        "        else:\n",
        "            response_text = full_response.strip()\n",
        "\n",
        "        return response_text if response_text else \"I could not generate a response. Please try again.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR in chat_with_assistant: {str(e)}]\")\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "print(\"[OK] Chat function defined (ULTRA FAST mode)\")\n",
        "print(\"Tip: Ask short, specific questions for the fastest replies.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f7nShRqvK0f",
        "outputId": "3b94a61d-a4cd-4047-e2a3-c39bd667f1e6"
      },
      "id": "2f7nShRqvK0f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Chat function defined (ULTRA FAST mode)\n",
            "Tip: Ask short, specific questions for the fastest replies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f7bd1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8f7bd1a",
        "outputId": "fa3ca88a-0ec0-4fb0-dbc0-b243d1be8167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Gradio interface created\n",
            "\n",
            "Launching interface...\n"
          ]
        }
      ],
      "source": [
        "# Create Gradio ChatInterface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_assistant,\n",
        "    title=\"Healthcare Assistant (Fine-Tuned TinyLlama)\",\n",
        "    description=\"Ask medical questions and get AI-powered answers. This model has been fine-tuned on medical flashcard data.\",\n",
        "    examples=[\n",
        "        \"What is the treatment for pneumonia?\",\n",
        "        \"What are the symptoms of diabetes?\",\n",
        "        \"How is hypertension diagnosed?\",\n",
        "        \"What causes anemia?\",\n",
        "        \"What is the function of insulin?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"[OK] Gradio interface created\")\n",
        "print(\"\\nLaunching interface...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7bdacd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "a7bdacd5",
        "outputId": "ca120e3c-92b6-4fba-ca6b-91916cd310ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://548ae98faef8dae442.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://548ae98faef8dae442.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "# Launch Gradio interface\n",
        "# In Colab: will create a public URL\n",
        "# In VS Code: will run on localhost\n",
        "demo.launch(share=True, debug=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f82276",
      "metadata": {
        "id": "96f82276"
      },
      "source": [
        "---\n",
        "## 11. Summary & Documentation\n",
        "\n",
        "### Project Summary\n",
        "- **Domain**: Healthcare (Medical Q&A)\n",
        "- **Model**: TinyLlama-1.1B-Chat-v1.0\n",
        "- **Dataset**: medalpaca/medical_meadow_medical_flashcards\n",
        "- **Training Samples**: 2,000\n",
        "- **Fine-Tuning Method**: LoRA (Parameter-Efficient Fine-Tuning)\n",
        "- **Trainable Parameters**: ~1-2% of total model parameters\n",
        "\n",
        "### Key Results\n",
        "Run the cells above to populate these metrics:\n",
        "- Training complete\n",
        "- Evaluation metrics calculated (BLEU, ROUGE, Perplexity)\n",
        "- Base vs fine-tuned comparison performed\n",
        "- Gradio UI deployed\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "74536fb416864ab99c152f2d5f2cc909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b810d861996f40c18169a26d1f985326",
              "IPY_MODEL_d992bcbd0b40459ab320e481c2caed37",
              "IPY_MODEL_357b2584712f4190a165120406398277"
            ],
            "layout": "IPY_MODEL_41e661994c5c45b4ab5ebd0b6503547e"
          }
        },
        "b810d861996f40c18169a26d1f985326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24924df5711f48fd96c91101ac543d8b",
            "placeholder": "​",
            "style": "IPY_MODEL_97233d832d804f1fa288562c5ca9fb59",
            "value": "Loading weights: 100%"
          }
        },
        "d992bcbd0b40459ab320e481c2caed37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e15bb23809b46589149bc461fb74be9",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc663443002c4e059eec3478a6ba67ff",
            "value": 201
          }
        },
        "357b2584712f4190a165120406398277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_279f88422e2d4ece889e307f436fa305",
            "placeholder": "​",
            "style": "IPY_MODEL_50df82cb05fc49f8b013ebf49738bf84",
            "value": " 201/201 [00:02&lt;00:00, 170.51it/s, Materializing param=model.norm.weight]"
          }
        },
        "41e661994c5c45b4ab5ebd0b6503547e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24924df5711f48fd96c91101ac543d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97233d832d804f1fa288562c5ca9fb59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e15bb23809b46589149bc461fb74be9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc663443002c4e059eec3478a6ba67ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "279f88422e2d4ece889e307f436fa305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50df82cb05fc49f8b013ebf49738bf84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6da899aef294c6ead3f3cd38ae4b0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d9d8fc6adcf48ca958fce2623fffe6c",
              "IPY_MODEL_76587f5265124091a0311af2095054a8",
              "IPY_MODEL_24f664be3be84f44b734851645e4299e"
            ],
            "layout": "IPY_MODEL_d6206e3ebbfb4ea79d7cbd55dc45cf6a"
          }
        },
        "0d9d8fc6adcf48ca958fce2623fffe6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a29e3594a6a45f38045519c1d7323f9",
            "placeholder": "​",
            "style": "IPY_MODEL_45df8d4eed7540d28b09438cce6fe1f8",
            "value": "Loading weights: 100%"
          }
        },
        "76587f5265124091a0311af2095054a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e088d67adfea46db8148328b0717c639",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1442e1104d24203b5092ece1bfd57d2",
            "value": 201
          }
        },
        "24f664be3be84f44b734851645e4299e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc34afd80a974e139bd8e566b5b32dba",
            "placeholder": "​",
            "style": "IPY_MODEL_f799de452f0e4f8195ba498c3ea9cf86",
            "value": " 201/201 [00:01&lt;00:00, 115.30it/s, Materializing param=model.norm.weight]"
          }
        },
        "d6206e3ebbfb4ea79d7cbd55dc45cf6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a29e3594a6a45f38045519c1d7323f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45df8d4eed7540d28b09438cce6fe1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e088d67adfea46db8148328b0717c639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1442e1104d24203b5092ece1bfd57d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc34afd80a974e139bd8e566b5b32dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f799de452f0e4f8195ba498c3ea9cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}