{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3789fe1",
      "metadata": {
        "id": "a3789fe1"
      },
      "source": [
        "# Healthcare Assistant via LLM Fine-Tuning\n",
        "\n",
        "## Project Overview\n",
        "This notebook fine-tunes **TinyLlama-1.1B-Chat** on medical flashcard Q&A data to create a domain-specific healthcare assistant.\n",
        "\n",
        "**Domain**: Healthcare (Medical Question Answering)  \n",
        "**Problem**: General LLMs lack specialized medical knowledge for accurate healthcare Q&A  \n",
        "**Solution**: Fine-tune TinyLlama on medical flashcards using LoRA for domain-specific accuracy\n",
        "\n",
        "**Key Features:**\n",
        "- **Domain**: Healthcare (Medical Q&A)\n",
        "- **Model**: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B parameters)\n",
        "- **Dataset**: medalpaca/medical_meadow_medical_flashcards (10,178 samples)\n",
        "- **Method**: LoRA (Low-Rank Adaptation) via PEFT library\n",
        "- **Platform**: Google Colab (Free T4 GPU)\n",
        "- **Training Samples**: 2,000 (validation: 200)\n",
        "\n",
        "**Assignment Goals:**\n",
        "1. Fine-tune a generative LLM for healthcare domain\n",
        "\n",
        "2. Implement parameter-efficient training (LoRA + 4-bit quantization)6. Document 3 hyperparameter experiments\n",
        "\n",
        "3. Evaluate with NLP metrics (BLEU, ROUGE, perplexity)5. Deploy with Gradio UI\n",
        "4. Compare base vs fine-tuned model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "d55a4645",
      "metadata": {
        "id": "d55a4645"
      },
      "source": [
        "---\n",
        "## 1. Environment Setup & Dependencies"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ab98fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ab98fe",
        "outputId": "cab0ada3-c7ff-4ebc-e529-5317f9f019a0"
      },
      "outputs": [],
      "source": [
        "# Check environment and GPU\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Verify we're in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    print(\"[OK] Running in Google Colab\")\n",
        "except:\n",
        "    print(\"[WARNING] Not running in Google Colab. This notebook is optimized for Colab.\")\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"[OK] GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"[INFO] No GPU detected. Training will use CPU (slower).\")\n",
        "    print(\"   Recommendation: Enable GPU in Runtime > Change runtime type > Hardware accelerator > T4 GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9981c6fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9981c6fb",
        "outputId": "8dd28543-9df7-411e-b48b-8abdf9e47ffd"
      },
      "outputs": [],
      "source": [
        "# Install required packages for Colab\n",
        "print(\"Installing dependencies...\\n\")\n",
        "\n",
        "!pip install -q -U datasets transformers accelerate peft bitsandbytes\n",
        "!pip install -q -U evaluate rouge-score sacrebleu gradio\n",
        "\n",
        "print(\"\\n[OK] All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"-q\", \"pyarrow\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"-q\", \"datasets\"])\n",
        "print(\"[OK] Fixed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H3qx25zeo-P",
        "outputId": "a17602f5-1175-4383-e53b-d2a3e4eb0278"
      },
      "id": "7H3qx25zeo-P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX: Aggressive PyArrow & Datasets compatibility resolver\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"AGGRESSIVE FIX: Resolving PyArrow binary compatibility issue...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Clear pip cache\n",
        "print(\"\\n[Step 1] Clearing pip cache...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"], capture_output=True)\n",
        "print(\"✓ Cache cleared\")\n",
        "\n",
        "# Step 2: Completely uninstall problematic packages\n",
        "print(\"\\n[Step 2] Uninstalling conflicting packages...\")\n",
        "for package in [\"pyarrow\", \"datasets\", \"transformers\", \"huggingface-hub\"]:\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package],\n",
        "        capture_output=True\n",
        "    )\n",
        "print(\"✓ Packages removed\")\n",
        "\n",
        "# Step 3: Reinstall with compatible versions\n",
        "print(\"\\n[Step 3] Installing compatible versions...\")\n",
        "packages_to_install = [\n",
        "    \"pyarrow==14.0.1\",  # Specific stable version\n",
        "    \"datasets>=2.17.0\",\n",
        "    \"huggingface-hub>=0.20.0\",\n",
        "    \"transformers>=4.36.0\"\n",
        "]\n",
        "\n",
        "for package in packages_to_install:\n",
        "    print(f\"  Installing {package}...\")\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        print(f\"  ⚠ Warning: {result.stderr[:100]}\")\n",
        "    else:\n",
        "        print(f\"  ✓ {package} installed\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"[OK] AGGRESSIVE FIX COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n IMPORTANT: Restart the kernel now!\")\n",
        "print(\"   In Colab: Runtime > Restart runtime\")\n",
        "print(\"   In VS Code: Select Restart Kernel button\")\n",
        "print(\"\\nThen run the 'Import libraries' cell after restart.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUuUtftnfayL",
        "outputId": "b6eaa786-ccab-4e9d-ac90-1b847b48b20a"
      },
      "id": "hUuUtftnfayL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93e744a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c93e744a",
        "outputId": "2fff9c9f-bf44-47da-def1-d38adbe35ccb"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"[OK] All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb137b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cb137b8",
        "outputId": "9d9a416f-0405-4c9e-a22e-3b6b0b44eca4"
      },
      "outputs": [],
      "source": [
        "# Check device (GPU or CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"[OK] Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"  [WARNING] Running on CPU. Training will be significantly slower.\")\n",
        "    print(\"  [RECOMMENDATION] Enable GPU: Runtime > Change runtime type > T4 GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b6c3e1",
      "metadata": {
        "id": "c7b6c3e1"
      },
      "source": [
        "---\n",
        "## 2. Dataset Loading & Exploration"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05fa59e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b05fa59e",
        "outputId": "93ee6699-0051-4f98-a177-b0b153817297"
      },
      "outputs": [],
      "source": [
        "# Load the medical flashcards dataset\n",
        "print(\"Loading dataset: medalpaca/medical_meadow_medical_flashcards\")\n",
        "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f7cee9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12f7cee9",
        "outputId": "c669703b-a763-433c-8567-bbbe64a29e96"
      },
      "outputs": [],
      "source": [
        "# Explore dataset\n",
        "print(\"Dataset info:\")\n",
        "print(f\"Total samples: {len(dataset['train'])}\")\n",
        "print(f\"\\nColumns: {dataset['train'].column_names}\")\n",
        "print(f\"\\nFirst 3 examples:\")\n",
        "\n",
        "for i in range(3):\n",
        "    example = dataset['train'][i]\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    for key, value in example.items():\n",
        "        print(f\"{key}: {value[:200] if isinstance(value, str) and len(value) > 200 else value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5225683b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "5225683b",
        "outputId": "8f31d762-d06a-4ac0-be88-8ea7ee35cdef"
      },
      "outputs": [],
      "source": [
        "# Analyze dataset statistics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get all text lengths\n",
        "text_lengths = []\n",
        "for item in dataset['train']:\n",
        "    # Adjust field name based on actual dataset structure\n",
        "    if 'input' in item and 'output' in item:\n",
        "        text_lengths.append(len(item['input']) + len(item['output']))\n",
        "    elif 'instruction' in item and 'output' in item:\n",
        "        text_lengths.append(len(item['instruction']) + len(item['output']))\n",
        "\n",
        "print(f\"Text length statistics:\")\n",
        "print(f\"Mean: {np.mean(text_lengths):.2f} characters\")\n",
        "print(f\"Median: {np.median(text_lengths):.2f} characters\")\n",
        "print(f\"Max: {np.max(text_lengths)} characters\")\n",
        "print(f\"Min: {np.min(text_lengths)} characters\")\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(text_lengths, bins=50, edgecolor='black')\n",
        "plt.xlabel('Text Length (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Text Lengths in Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a737f061",
      "metadata": {
        "id": "a737f061"
      },
      "source": [
        "---\n",
        "## 3. Data Preprocessing & Formatting"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1be70b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1be70b7",
        "outputId": "74be7780-83c5-43f3-d4a5-7c6632fa3239"
      },
      "outputs": [],
      "source": [
        "# Define prompt template for instruction-following format\n",
        "def create_prompt_template(instruction, response=\"\"):\n",
        "    \"\"\"\n",
        "    Create a chat-style prompt for TinyLlama.\n",
        "    Format: <|system|>\\n{system_message}\\n<|user|>\\n{instruction}\\n<|assistant|>\\n{response}\n",
        "    \"\"\"\n",
        "    system_message = \"You are a helpful medical assistant. Provide accurate and concise answers to medical questions.\"\n",
        "\n",
        "    if response:\n",
        "        # Training format (with answer)\n",
        "        prompt = f\"\"\"<|system|>\n",
        "{system_message}\n",
        "<|user|>\n",
        "{instruction}\n",
        "<|assistant|>\n",
        "{response}\"\"\"\n",
        "    else:\n",
        "        # Inference format (without answer)\n",
        "        prompt = f\"\"\"<|system|>\n",
        "{system_message}\n",
        "<|user|>\n",
        "{instruction}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Test the template\n",
        "sample = dataset['train'][0]\n",
        "# Adjust field names based on actual dataset structure\n",
        "instruction_field = 'input' if 'input' in sample else 'instruction'\n",
        "output_field = 'output'\n",
        "\n",
        "test_prompt = create_prompt_template(sample[instruction_field], sample[output_field])\n",
        "print(\"Sample formatted prompt:\")\n",
        "print(test_prompt)\n",
        "print(f\"\\nPrompt length: {len(test_prompt)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c73f23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c73f23",
        "outputId": "d4e1212d-0f66-4e6b-990c-bb559bcf0038"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "print(f\"Loading tokenizer: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"[OK] Tokenizer loaded\")\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Max length: {tokenizer.model_max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22dca03f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dca03f",
        "outputId": "5e50dabd-032c-4125-a66a-3f622add2fcf"
      },
      "outputs": [],
      "source": [
        "# Preprocess dataset\n",
        "def format_dataset(examples):\n",
        "    \"\"\"Format dataset into instruction-response pairs.\"\"\"\n",
        "    # Determine field names\n",
        "    sample_keys = list(examples.keys())\n",
        "    instruction_field = 'input' if 'input' in sample_keys else 'instruction'\n",
        "    output_field = 'output'\n",
        "\n",
        "    prompts = []\n",
        "    for i in range(len(examples[instruction_field])):\n",
        "        instruction = examples[instruction_field][i]\n",
        "        response = examples[output_field][i]\n",
        "        prompt = create_prompt_template(instruction, response)\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    return {\"text\": prompts}\n",
        "\n",
        "# Apply formatting\n",
        "print(\"Formatting dataset...\")\n",
        "formatted_dataset = dataset.map(\n",
        "    format_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names\n",
        ")\n",
        "\n",
        "print(\"[OK] Dataset formatted\")\n",
        "print(f\"Sample formatted text:\\n{formatted_dataset['train'][0]['text'][:500]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861eb7b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "861eb7b2",
        "outputId": "9f3ec11c-5b75-432e-ff05-9063087ec997"
      },
      "outputs": [],
      "source": [
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the formatted prompts.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,  # Limit to 512 tokens for faster training\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = formatted_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"[OK] Dataset tokenized\")\n",
        "print(f\"Sample tokenized length: {len(tokenized_dataset['train'][0]['input_ids'])} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f4a46c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11f4a46c",
        "outputId": "e4daeec0-c6b5-4759-a530-2a07b155d5d3"
      },
      "outputs": [],
      "source": [
        "# Split dataset: use 2000 samples for training, 200 for validation\n",
        "# This balances quality with training time on free Colab\n",
        "train_size = 2000\n",
        "val_size = 200\n",
        "\n",
        "if len(tokenized_dataset['train']) < train_size + val_size:\n",
        "    train_size = int(len(tokenized_dataset['train']) * 0.9)\n",
        "    val_size = len(tokenized_dataset['train']) - train_size\n",
        "\n",
        "train_dataset = tokenized_dataset['train'].select(range(train_size))\n",
        "eval_dataset = tokenized_dataset['train'].select(range(train_size, train_size + val_size))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18c85a8",
      "metadata": {
        "id": "f18c85a8"
      },
      "source": [
        "---\n",
        "## 4. Model Loading with LoRA Configuration"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b17e2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "74536fb416864ab99c152f2d5f2cc909",
            "b810d861996f40c18169a26d1f985326",
            "d992bcbd0b40459ab320e481c2caed37",
            "357b2584712f4190a165120406398277",
            "41e661994c5c45b4ab5ebd0b6503547e",
            "24924df5711f48fd96c91101ac543d8b",
            "97233d832d804f1fa288562c5ca9fb59",
            "6e15bb23809b46589149bc461fb74be9",
            "fc663443002c4e059eec3478a6ba67ff",
            "279f88422e2d4ece889e307f436fa305",
            "50df82cb05fc49f8b013ebf49738bf84"
          ]
        },
        "id": "f4b17e2a",
        "outputId": "fa5d0475-6e27-46ee-9e1f-0353ed4cc5cf"
      },
      "outputs": [],
      "source": [
        "# Load base model (with 4-bit quantization on GPU, full precision on CPU)\n",
        "print(f\"Loading base model: {model_name}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using 4-bit quantization for memory efficiency on GPU\")\n",
        "    from transformers import BitsAndBytesConfig\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load model with quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Prepare for LoRA training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    print(\"[OK] Model loaded with 4-bit quantization\")\n",
        "else:\n",
        "    print(\"Using full precision on CPU\")\n",
        "    # Load model without quantization for CPU\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"[OK] Model loaded in full precision\")\n",
        "\n",
        "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1653518c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1653518c",
        "outputId": "f640995b-fd9f-44a7-ee45-af27bb871250"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "print(\"Configuring LoRA...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # LoRA rank (higher = more parameters, better quality but slower)\n",
        "    lora_alpha=32,  # LoRA scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply LoRA to attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"[OK] LoRA configured\")\n",
        "print(f\"Trainable parameters: {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Total parameters: {total_params / 1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbccfee7",
      "metadata": {
        "id": "bbccfee7"
      },
      "source": [
        "---\n",
        "## 5. Training Configuration & Experiments\n",
        "\n",
        "We will run **3 experiments** with different hyperparameters to find the best configuration."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2290aef7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2290aef7",
        "outputId": "842f0cc1-50bc-4696-acc6-2bfca9adc38c"
      },
      "outputs": [],
      "source": [
        "# Define experiment configurations\n",
        "experiments = {\n",
        "    \"Experiment 1: Baseline\": {\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"num_epochs\": 1,\n",
        "        \"batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_steps\": 100,\n",
        "    },\n",
        "    \"Experiment 2: Lower LR\": {\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"num_epochs\": 2,\n",
        "        \"batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_steps\": 100,\n",
        "    },\n",
        "    \"Experiment 3: Optimized\": {\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"num_epochs\": 2,\n",
        "        \"batch_size\": 4,\n",
        "        \"gradient_accumulation_steps\": 2,\n",
        "        \"warmup_steps\": 50,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display experiments\n",
        "print(\"Experiment Configurations:\")\n",
        "print(\"=\"*70)\n",
        "for exp_name, config in experiments.items():\n",
        "    print(f\"\\n{exp_name}\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c5572f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c5572f",
        "outputId": "38005094-cbf8-43d7-b729-f51804e8fb2b"
      },
      "outputs": [],
      "source": [
        "# Choose experiment to run (change this to run different experiments)\n",
        "CURRENT_EXPERIMENT = \"Experiment 1: Baseline\"  # Change to test different configs\n",
        "\n",
        "config = experiments[CURRENT_EXPERIMENT]\n",
        "print(f\"Running: {CURRENT_EXPERIMENT}\")\n",
        "print(f\"Configuration: {config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871414a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "871414a2",
        "outputId": "b3b5caef-e2e6-4591-aea7-1320e58ad4bb"
      },
      "outputs": [],
      "source": [
        "# Set up training arguments\n",
        "output_dir = f\"./results_{CURRENT_EXPERIMENT.split(':')[0].replace(' ', '_').lower()}\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=config[\"num_epochs\"],\n",
        "    per_device_train_batch_size=config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=config[\"batch_size\"],\n",
        "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "    learning_rate=config[\"learning_rate\"],\n",
        "    warmup_steps=config[\"warmup_steps\"],\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),  # Use FP16 on GPU, FP32 on CPU\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "print(\"[OK] Training arguments configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67183b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a67183b6",
        "outputId": "55581d55-1f7e-4755-e2eb-7f093c77b5ce"
      },
      "outputs": [],
      "source": [
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal LM, not masked LM\n",
        ")\n",
        "\n",
        "print(\"[OK] Data collator ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a679b4b5",
      "metadata": {
        "id": "a679b4b5"
      },
      "source": [
        "---\n",
        "## 6. Training"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e33a2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61e33a2a",
        "outputId": "f85d5b40-ab15-4d53-c3be-cf42f928961c"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"[OK] Trainer initialized\")\n",
        "print(f\"Training will start on device: {training_args.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a3bee39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "0a3bee39",
        "outputId": "409e6a8b-5b6f-4977-d9d4-ea529d701de2"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Starting training: {CURRENT_EXPERIMENT}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Track GPU memory before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    memory_before = torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Track GPU memory after training\n",
        "if torch.cuda.is_available():\n",
        "    memory_peak = torch.cuda.max_memory_allocated() / 1e9\n",
        "    print(f\"\\n[OK] Training completed!\")\n",
        "    print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "    print(f\"Peak GPU memory: {memory_peak:.2f} GB\")\n",
        "else:\n",
        "    print(f\"\\n[OK] Training completed!\")\n",
        "    print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "\n",
        "# Save training metrics\n",
        "metrics = train_result.metrics\n",
        "print(f\"\\nFinal training loss: {metrics['train_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20ae922",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a20ae922",
        "outputId": "65e3a717-b87b-4dbe-ae40-e1697e5a626e"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model_save_path = f\"./fine_tuned_model_{CURRENT_EXPERIMENT.split(':')[0].replace(' ', '_').lower()}\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"[OK] Model saved to: {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9e332a",
      "metadata": {
        "id": "5f9e332a"
      },
      "source": [
        "---\n",
        "## 7. Evaluation with NLP Metrics"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deac240a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deac240a",
        "outputId": "25cf3149-fe33-4647-ba79-3876ac0b66ce"
      },
      "outputs": [],
      "source": [
        "# Load evaluation metrics\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "print(\"[OK] Evaluation metrics loaded (BLEU, ROUGE)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d288bf22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d288bf22",
        "outputId": "2d9cb499-406d-4fb0-e5bd-ecb3a9551bfb"
      },
      "outputs": [],
      "source": [
        "# Generate predictions on validation set\n",
        "print(\"Generating predictions for evaluation...\")\n",
        "\n",
        "num_eval_samples = 50  # Evaluate on subset for speed\n",
        "eval_samples = eval_dataset.select(range(min(num_eval_samples, len(eval_dataset))))\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, sample in enumerate(eval_samples):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processing {i}/{num_eval_samples}...\")\n",
        "\n",
        "        # Get input without the response\n",
        "        input_ids = torch.tensor([sample['input_ids'][:256]]).to(model.device)  # Use first half as input\n",
        "\n",
        "        # Generate\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        reference = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        references.append(reference)\n",
        "\n",
        "print(\"[OK] Predictions generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0747a87c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0747a87c",
        "outputId": "11ab48fd-cb23-4bfe-9154-b4c077334a88"
      },
      "outputs": [],
      "source": [
        "# Calculate BLEU score\n",
        "bleu_results = bleu_metric.compute(\n",
        "    predictions=predictions,\n",
        "    references=[[ref] for ref in references]\n",
        ")\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_results = rouge_metric.compute(\n",
        "    predictions=predictions,\n",
        "    references=references\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBLEU Score: {bleu_results['score']:.2f}\")\n",
        "print(f\"\\nROUGE Scores:\")\n",
        "print(f\"  ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
        "print(f\"  ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
        "print(f\"  ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5039b83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "d5039b83",
        "outputId": "b86d1c70-d583-41fb-ba80-9e31ae6d07c7"
      },
      "outputs": [],
      "source": [
        "# Calculate perplexity on validation set\n",
        "print(\"\\nCalculating perplexity...\")\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "perplexity = np.exp(eval_results['eval_loss'])\n",
        "\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28b6788",
      "metadata": {
        "id": "a28b6788"
      },
      "source": [
        "---\n",
        "## 8. Qualitative Testing & Comparison"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26299ca3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26299ca3",
        "outputId": "b36cda8b-a877-4600-9b0d-4b3662603fc0"
      },
      "outputs": [],
      "source": [
        "# Test questions for qualitative evaluation\n",
        "test_questions = [\n",
        "    \"What is the treatment for pneumonia?\",\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How is hypertension diagnosed?\",\n",
        "    \"What causes anemia?\",\n",
        "    \"What is the function of insulin?\",\n",
        "]\n",
        "\n",
        "print(\"Qualitative Testing: Fine-Tuned Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for question in test_questions:\n",
        "    prompt = create_prompt_template(question, \"\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response\n",
        "    if \"<|assistant|>\" in response:\n",
        "        answer = response.split(\"<|assistant|>\")[-1].strip()\n",
        "    else:\n",
        "        answer = response\n",
        "\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918edd2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "f6da899aef294c6ead3f3cd38ae4b0b6",
            "0d9d8fc6adcf48ca958fce2623fffe6c",
            "76587f5265124091a0311af2095054a8",
            "24f664be3be84f44b734851645e4299e",
            "d6206e3ebbfb4ea79d7cbd55dc45cf6a",
            "0a29e3594a6a45f38045519c1d7323f9",
            "45df8d4eed7540d28b09438cce6fe1f8",
            "e088d67adfea46db8148328b0717c639",
            "f1442e1104d24203b5092ece1bfd57d2",
            "cc34afd80a974e139bd8e566b5b32dba",
            "f799de452f0e4f8195ba498c3ea9cf86"
          ]
        },
        "id": "918edd2a",
        "outputId": "2f2277d4-fafc-44e8-d945-aa932f87e92d"
      },
      "outputs": [],
      "source": [
        "# Load base model for comparison (without fine-tuning)\n",
        "print(\"Loading base model for comparison...\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "else:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "print(\"[OK] Base model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a23a91b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a23a91b7",
        "outputId": "48c1a8d0-0088-4bb0-9f86-96096cc4bc26"
      },
      "outputs": [],
      "source": [
        "# Compare base vs fine-tuned\n",
        "print(\"\\nComparison: Base Model vs Fine-Tuned Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_question = \"What is the treatment for pneumonia?\"\n",
        "\n",
        "# Base model response\n",
        "prompt = create_prompt_template(comparison_question, \"\")\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "outputs_base = base_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "response_base = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n",
        "if \"<|assistant|>\" in response_base:\n",
        "    answer_base = response_base.split(\"<|assistant|>\")[-1].strip()\n",
        "else:\n",
        "    answer_base = response_base\n",
        "\n",
        "# Fine-tuned model response\n",
        "outputs_finetuned = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "response_finetuned = tokenizer.decode(outputs_finetuned[0], skip_special_tokens=True)\n",
        "if \"<|assistant|>\" in response_finetuned:\n",
        "    answer_finetuned = response_finetuned.split(\"<|assistant|>\")[-1].strip()\n",
        "else:\n",
        "    answer_finetuned = response_finetuned\n",
        "\n",
        "print(f\"\\nQuestion: {comparison_question}\")\n",
        "print(f\"\\n--- BASE MODEL ---\")\n",
        "print(answer_base)\n",
        "print(f\"\\n--- FINE-TUNED MODEL ---\")\n",
        "print(answer_finetuned)\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a43ae46",
      "metadata": {
        "id": "9a43ae46"
      },
      "source": [
        "---\n",
        "## 9. Experiment Tracking Table\n",
        "\n",
        "Document all experiments and their results for the report."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc61679",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fc61679",
        "outputId": "10a06823-8da1-429f-a8e7-6a8affe27c31"
      },
      "outputs": [],
      "source": [
        "# Create experiment tracking table\n",
        "experiment_results = {\n",
        "    \"Experiment\": [CURRENT_EXPERIMENT],\n",
        "    \"Learning Rate\": [config[\"learning_rate\"]],\n",
        "    \"Epochs\": [config[\"num_epochs\"]],\n",
        "    \"Batch Size\": [config[\"batch_size\"]],\n",
        "    \"Gradient Accumulation\": [config[\"gradient_accumulation_steps\"]],\n",
        "    \"Training Loss\": [f\"{metrics['train_loss']:.4f}\"],\n",
        "    \"Validation Loss\": [f\"{eval_results['eval_loss']:.4f}\"],\n",
        "    \"Perplexity\": [f\"{perplexity:.2f}\"],\n",
        "    \"BLEU\": [f\"{bleu_results['score']:.2f}\"],\n",
        "    \"ROUGE-1\": [f\"{rouge_results['rouge1']:.4f}\"],\n",
        "    \"ROUGE-L\": [f\"{rouge_results['rougeL']:.4f}\"],\n",
        "    \"Training Time (min)\": [f\"{training_time/60:.2f}\"],\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    experiment_results[\"Peak GPU Memory (GB)\"] = [f\"{memory_peak:.2f}\"]\n",
        "\n",
        "experiment_df = pd.DataFrame(experiment_results)\n",
        "\n",
        "print(\"\\nEXPERIMENT TRACKING TABLE\")\n",
        "print(\"=\"*100)\n",
        "print(experiment_df.to_string(index=False))\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Save to CSV\n",
        "experiment_df.to_csv(\"experiment_results.csv\", mode='a', header=not pd.io.common.file_exists(\"experiment_results.csv\"), index=False)\n",
        "print(\"\\n[OK] Results saved to experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21038d24",
      "metadata": {
        "id": "21038d24"
      },
      "source": [
        "---\n",
        "## 10. Gradio UI Deployment\n",
        "\n",
        "Create an interactive web interface for the healthcare assistant."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a006df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25a006df",
        "outputId": "a0fbbc74-9cb5-462e-e606-c6eaa3cc259e"
      },
      "outputs": [],
      "source": [
        "# Install Gradio if not already installed\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    import gradio as gr\n",
        "    print(\"[OK] Gradio is already installed\")\n",
        "except:\n",
        "    print(\"Installing Gradio...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gradio\"])\n",
        "    import gradio as gr\n",
        "    print(\"[OK] Gradio installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chat function for Gradio - ULTRA FAST VERSION\n",
        "# Purpose: fastest possible responses (short + direct)\n",
        "\n",
        "def chat_with_assistant(message, history):\n",
        "    try:\n",
        "        model.eval()\n",
        "        model.config.use_cache = True\n",
        "\n",
        "        # Short prompt for speed\n",
        "        prompt = create_prompt_template(message, \"\")\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=256  # shorter context = faster\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = inputs.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # Ultra-fast decoding (greedy, short)\n",
        "        with torch.inference_mode():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=50,    # ultra short\n",
        "                min_new_tokens=10,\n",
        "                do_sample=False,      # greedy decoding (fastest)\n",
        "                temperature=0.0,\n",
        "                top_p=1.0,\n",
        "                top_k=0,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=2,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \"<|assistant|>\" in full_response:\n",
        "            response_text = full_response.split(\"<|assistant|>\")[-1].strip()\n",
        "        else:\n",
        "            response_text = full_response.strip()\n",
        "\n",
        "        return response_text if response_text else \"I could not generate a response. Please try again.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR in chat_with_assistant: {str(e)}]\")\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "print(\"[OK] Chat function defined (ULTRA FAST mode)\")\n",
        "print(\"Tip: Ask short, specific questions for the fastest replies.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f7nShRqvK0f",
        "outputId": "3b94a61d-a4cd-4047-e2a3-c39bd667f1e6"
      },
      "id": "2f7nShRqvK0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f7bd1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8f7bd1a",
        "outputId": "fa3ca88a-0ec0-4fb0-dbc0-b243d1be8167"
      },
      "outputs": [],
      "source": [
        "# Create Gradio ChatInterface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_assistant,\n",
        "    title=\"Healthcare Assistant (Fine-Tuned TinyLlama)\",\n",
        "    description=\"Ask medical questions and get AI-powered answers. This model has been fine-tuned on medical flashcard data.\",\n",
        "    examples=[\n",
        "        \"What is the treatment for pneumonia?\",\n",
        "        \"What are the symptoms of diabetes?\",\n",
        "        \"How is hypertension diagnosed?\",\n",
        "        \"What causes anemia?\",\n",
        "        \"What is the function of insulin?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"[OK] Gradio interface created\")\n",
        "print(\"\\nLaunching interface...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7bdacd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "a7bdacd5",
        "outputId": "ca120e3c-92b6-4fba-ca6b-91916cd310ec"
      },
      "outputs": [],
      "source": [
        "# Launch Gradio interface\n",
        "# In Colab: will create a public URL\n",
        "# In VS Code: will run on localhost\n",
        "demo.launch(share=True, debug=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f82276",
      "metadata": {
        "id": "96f82276"
      },
      "source": [
        "---\n",
        "## 11. Summary & Documentation\n",
        "\n",
        "### Project Summary\n",
        "- **Domain**: Healthcare (Medical Q&A)\n",
        "- **Model**: TinyLlama-1.1B-Chat-v1.0\n",
        "- **Dataset**: medalpaca/medical_meadow_medical_flashcards\n",
        "- **Training Samples**: 2,000\n",
        "- **Fine-Tuning Method**: LoRA (Parameter-Efficient Fine-Tuning)\n",
        "- **Trainable Parameters**: ~1-2% of total model parameters\n",
        "\n",
        "### Key Results\n",
        "Run the cells above to populate these metrics:\n",
        "- Training complete\n",
        "- Evaluation metrics calculated (BLEU, ROUGE, Perplexity)\n",
        "- Base vs fine-tuned comparison performed\n",
        "- Gradio UI deployed\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}